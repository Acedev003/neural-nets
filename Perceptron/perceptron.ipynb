{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, input_size: int, learning_rate: float, activation_function:dict):\n",
    "        self.input_size    = input_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.weights    = [random.uniform(-1, 1) for _ in range(input_size)]\n",
    "        self.bias       = random.uniform(-1, 1)\n",
    "        self.activation = activation_function\n",
    "    \n",
    "    def train(self, inputs, targets, epochs,verbose=False):    \n",
    "        if len(inputs) != len(targets):\n",
    "            raise ValueError('Inputs and targets must be of the same length')\n",
    "        \n",
    "        for _ in range(epochs):\n",
    "            for input_vector, target in zip(inputs, targets):\n",
    "                output              = self.forward_pass(input_vector)\n",
    "                weighted_sum        = output['weighted_sum']\n",
    "                activated_output    = output['activation']\n",
    "                activation_gradient = self.activation['activation_diff'](weighted_sum)          # dz/da\n",
    "                loss                = (activated_output - target[0]) ** 2\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"Loss: {loss:.7f}, Weights: {self.weights}\")\n",
    "\n",
    "                # Update weights\n",
    "                for i in range(self.input_size):\n",
    "                    self.weights[i] -= self.learning_rate * (input_vector[i] * activation_gradient * 2 * (activated_output - target[0]))\n",
    "                \n",
    "                # Update bias\n",
    "                self.bias -= self.learning_rate * (activation_gradient  * 2 * (activated_output - target[0]))\n",
    "            \n",
    "        print(f\"Final loss: {loss:.7f}, Weights: {self.weights}\")\n",
    "    \n",
    "    def forward_pass(self, input_vector):\n",
    "        if len(input_vector) != self.input_size:\n",
    "            raise ValueError('Input vector size does not match expected input size')\n",
    "        \n",
    "        weighted_sum = sum(x * w for x, w in zip(input_vector, self.weights)) + self.bias\n",
    "        activated_output = self.activation['activation'](weighted_sum)\n",
    "        return {'weighted_sum': weighted_sum, 'activation': activated_output}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Functions along with their derivative versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid Activation and its derivative\n",
    "\n",
    "def sigmoid(x): return 1 / (1 + math.exp(-x))\n",
    "def sigmoid_diff(x): return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "# Linear Activation and its derivative\n",
    "\n",
    "def linear(x): return x\n",
    "def linear_diff(x): return 1\n",
    "\n",
    "# ReLU Activation and its derivative\n",
    "\n",
    "def relu (x): return max(0,x)\n",
    "def relu_diff(x): return 0 if x<=0 else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1 : Linear Function 2x+10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "\n",
    "X = [[0],[1],[2],[3],[4],[5],[6],[7]]\n",
    "Y = [[10],[12],[14],[16],[18],[20],[22],[24]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "\n",
    "neuron = Perceptron(1,0.05,{'activation':linear,'activation_diff':linear_diff})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.033793675264269"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron.forward_pass([10])['activation']  # Wrong Answer :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss: 0.0000000, Weights: [2.0000562926222174]\n"
     ]
    }
   ],
   "source": [
    "# Train for 200 epochs\n",
    "\n",
    "neuron.train(X,Y,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.00102008957644"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron.forward_pass([20])['activation']       # Correct Answer :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2 : Function 2a+5b-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "\n",
    "X = [[0,0],[0,1],[1,1],[2,2],[5,5],[10,10],[1,50],[20,1],[3,4],[9,1],[3,8],[4,0]]\n",
    "Y = [[-5],[0],[2],[9],[30],[65],[247],[40],[21],[18],[41],[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "\n",
    "neuron = Perceptron(2,0.01,{'activation':linear,'activation_diff':linear_diff})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.029952426216928174"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron.forward_pass([0,0])['activation']  # Wrong Answer :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 123920082453127336591816409875177158956311880733623012984641026425608596112338275847948865917221951286978394353690519024407990290611580637834295902755372007944764088137002768215317700630268992784030826880960757314615542890302845473202118218857352639745719601807066742488675896640553225316408601618895863808.0000000, Weights: [-4.262681643420455e+153, 4.196719318988435e+153]\n",
      "Loss: 14835877394943382384181384432340824409063052867158561137945380342957930509984125726244430369398548491790146431015378322515493513309421532443839299676925461120869160693611303240428376715191851305849162678442315294677990612873515735699281055203477102135186616253006378541055692243081816626166460074825397305344.0000000, Weights: [-4.262681643420455e+153, 4.196719318988435e+153]\n",
      "Loss: 319241033206021918154517820435716484287377116851107886960656164574951764704961923199098839880571456822846235448041482907730059388267373060016618393877564895477913204435814519686234523504567603160329577247874896927015208155297391035102249819954078408675379304458380518442799806464187673367912421959713947648.0000000, Weights: [-4.262681643420455e+153, 4.1196845803442536e+153]\n",
      "Loss: 424465062138621050893312148840480346195114109077732297419692741088522413149185588507739475870654748906090290956412615074176440182530095179524789639661113117096698473243788998744951568623318468950442037639804313823492168397908254390664776287326423901922895489053301440729109758429282245065111357183915720704.0000000, Weights: [-4.25138135965046e+153, 4.130984864114248e+153]\n",
      "Loss: 546217354463252438128890417474673656821281820006521190399775986272844495504571636096128309074730415233473718278321370291450094601127351670645170768595003711985146622729569737868938967999418612455493633479489029717811915737200201232348269804519371005806607710035468945083003276365916065748080168002060812288.0000000, Weights: [-4.225320966341421e+153, 4.157045257423287e+153]\n",
      "Loss: 170129808779708028680933454828817737103378191203971383834643664587467914794325241984798458938572874513197449595479770462502842110261598657440725206044689358643151674206682037035533670749969390440467276067374176167289406178763449163929446991362139065584407787570609455378555295755302367341422715311153479680.0000000, Weights: [-4.1514144473824515e+153, 4.230951776382257e+153]\n"
     ]
    },
    {
     "ename": "OverflowError",
     "evalue": "(34, 'Numerical result out of range')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2957193/2434553568.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train for 5000 epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mneuron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2957193/529309058.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, inputs, targets, epochs, verbose)\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0mactivated_output\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'activation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mactivation_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'activation_diff'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweighted_sum\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;31m# dz/da\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                 \u001b[0mloss\u001b[0m                \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mactivated_output\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOverflowError\u001b[0m: (34, 'Numerical result out of range')"
     ]
    }
   ],
   "source": [
    "# Train for 5000 epochs\n",
    "\n",
    "neuron.train(X,Y,5000,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is seen above is a phenomenon known as **exploding gradients**. Notice how the weights kept \"exploding\" until we could no longer hold that number in mem0ry. An opposite scenario to this is **vanishing gradients** were the gradients end up being infinitely small. Check out [this](https://youtu.be/2f_45VzKEfE?si=UjNDSgIEBSgOBzhs) video that shows why this happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss: 0.0000000, Weights: [1.999999175795818, 4.999999784037885]\n"
     ]
    }
   ],
   "source": [
    "neuron = Perceptron(2,0.0002,{'activation':linear,'activation_diff':linear_diff})\n",
    "\n",
    "neuron.train(X,Y,6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "694.9999061451219"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron.forward_pass([100,100])['activation']        # Almost Correct Answer :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 3: XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[0,0],[0,1],[1,0],[1,1]]\n",
    "Y = [[0],[1],[1],[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss: 0.6564842, Weights: [0.530207017370598, 0.5690668017135673]\n"
     ]
    }
   ],
   "source": [
    "neuron = Perceptron(2,0.0002,{'activation':sigmoid,'activation_diff':sigmoid})\n",
    "\n",
    "neuron.train(X,Y,1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8101159779991794"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron.forward_pass([1,1])['activation'] # Fail :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One notable observation is how the perceptron completely fails when attempting to predict a non-linear function, like XOR. Some articles mention that a single perceptron cannot predict XOR because it can only represent linear boundaries. But then you might ask, what is the purpose of the activation function? Wasn't it supposed to introduce non-linearity?\n",
    "\n",
    "The truth is that it depends; different activation functions introduce non-linearity to only a certain extent or are limited to specific hyperplanes. [This](https://medium.com/@lucaspereira0612/solving-xor-with-a-single-perceptron-34539f395182) article by Lucas Araújo by Lucas Araújo shows how a parameterized sigmoid function can solve the XOR problem using a single perceptron.\n",
    "\n",
    "Also we have these activation functions introduced recently \n",
    "- https://arxiv.org/pdf/2108.12943\n",
    "- https://arxiv.org/pdf/2409.10821\n",
    "\n",
    "\n",
    "But the main takeaway is that as we add more layers (or more deep) the neural-network is able to learning features effectively, with initial layers learning low-level features and later layers of neurons handling higher features in data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
